{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b689dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\python39\\lib\\site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.1 MB 1.7 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.1/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/2.1 MB 1.4 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.3/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.4/2.1 MB 1.4 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.5/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.5/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.6/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.6/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.7/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.7/2.1 MB 1.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 0.8/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 0.9/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.0/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.0/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 1.0/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 1.0/2.1 MB 988.7 kB/s eta 0:00:02\n",
      "     -------------------- ------------------- 1.1/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.1 MB 978.8 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 992.1 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 992.1 kB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.1 MB 946.0 kB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.2/2.1 MB 963.3 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.3/2.1 MB 937.8 kB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.3/2.1 MB 937.8 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.3/2.1 MB 929.6 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.4/2.1 MB 955.6 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.5/2.1 MB 983.2 kB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.6/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.7/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.8/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.8/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.9/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.9/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ac1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/1.5 MB 656.4 kB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.1/1.5 MB 871.5 kB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 871.5 kB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 807.1 kB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 737.3 kB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.5 MB 793.0 kB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.5 MB 793.0 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 733.3 kB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.3/1.5 MB 724.7 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.3/1.5 MB 746.0 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 731.4 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.4/1.5 MB 726.4 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.5/1.5 MB 721.8 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.5/1.5 MB 721.8 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 683.6 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 696.6 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 696.6 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.6/1.5 MB 643.0 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.6/1.5 MB 643.0 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 677.5 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 677.5 kB/s eta 0:00:02\n",
      "     ------------------ --------------------- 0.7/1.5 MB 685.4 kB/s eta 0:00:02\n",
      "     ------------------- -------------------- 0.7/1.5 MB 683.9 kB/s eta 0:00:02\n",
      "     -------------------- ------------------- 0.8/1.5 MB 691.8 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 0.8/1.5 MB 709.0 kB/s eta 0:00:01\n",
      "     --------------------- ------------------ 0.8/1.5 MB 689.6 kB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.0/1.5 MB 744.1 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 758.8 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 755.7 kB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.0/1.5 MB 412.1 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.0/1.5 MB 412.1 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.0/1.5 MB 412.1 kB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 490.4 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.3/1.5 MB 489.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 494.2 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 493.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.5/1.5 MB 510.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.5/1.5 MB 510.6 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 514.3 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 505.4 kB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/268.0 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 41.0/268.0 kB 960.0 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 112.6/268.0 kB 1.1 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 194.6/268.0 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 225.3/268.0 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 256.0/268.0 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 268.0/268.0 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.6 kB ? eta -:--:--\n",
      "     ------------ --------------------------- 30.7/96.6 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\python39\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/77.1 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 71.7/77.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 77.1/77.1 kB 718.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\91939\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5df4f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8574e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\91939\\Downloads\\IMDB Dataset.csv.zip')\n",
    "print(data)\n",
    "print(data.head())\n",
    "print(data['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb2b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the data\n",
    "def clean_text(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Vectorize the data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfc87c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build a model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train_vec.shape[1],)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5492ff87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(random_state=42, tol=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(random_state=42, tol=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(random_state=42, tol=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 5: Train the model\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, random_state=42, max_iter=1000, tol=None)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c21f3d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model on the test set\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa299d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Make an end-to-end model\n",
    "def predict_sentiment(review):\n",
    "    review_vec = vectorizer.transform([review])\n",
    "    sentiment =  clf.predict(review_vec)[0][0]\n",
    "    if sentiment == 1:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "print(predict_sentiment('This movie was amazing!'))\n",
    "print(predict_sentiment('This movie was terrible.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674297e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\91939\\Downloads\\IMDB Dataset.csv.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b820e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb02923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Fultonwords' from 'nltk.corpus' (C:\\python39\\lib\\site-packages\\nltk\\corpus\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Fultonwords\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Remove HTML tags\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<.*?>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Fultonwords' from 'nltk.corpus' (C:\\python39\\lib\\site-packages\\nltk\\corpus\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import Fultonwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "    return text\n",
    "\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c376e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293ce394",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\91939/nltk_data'\n    - 'C:\\\\python39\\\\nltk_data'\n    - 'C:\\\\python39\\\\share\\\\nltk_data'\n    - 'C:\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\91939\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\91939/nltk_data'\n    - 'C:\\\\python39\\\\nltk_data'\n    - 'C:\\\\python39\\\\share\\\\nltk_data'\n    - 'C:\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\91939\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_list])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m stopwords_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_list])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\91939/nltk_data'\n    - 'C:\\\\python39\\\\nltk_data'\n    - 'C:\\\\python39\\\\share\\\\nltk_data'\n    - 'C:\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\91939\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords_list])\n",
    "    return text\n",
    "\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd8281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
